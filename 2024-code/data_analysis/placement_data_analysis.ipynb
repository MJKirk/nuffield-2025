{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Physics Searches at the Large Hadron Collider - Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps needed to make your comparison of Standard Model predictions to **simulated** data for Higgs boson production are described in this notebook.\n",
    "\n",
    "The data we examine are for an experimental analysis for the process $pp\\rightarrow Hjj$ where $j$ represents a jet of hadrons. We aim to produce a distribution examining the mass of the two combined jets:\n",
    "\n",
    "$$\\frac{\\mathrm{d}\\sigma}{\\mathrm{d}m_{jj}},$$\n",
    "\n",
    "which is measured in units of $\\mathrm{pb/GeV}$. This will plotted in bins of $m_{jj}$, measured in $\\mathrm{GeV}$.\n",
    "\n",
    "In this notebook you will plot the Standard Model background against the experimental data to examine whether we can confidently say the data indicates the presence of new physics.\n",
    "\n",
    "For further information on the background and project brief please refer to the project [webpage](https://ippp.dur.ac.uk/~mkirk/nuffield/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information can be stored in many form on computing systems, **data** needs to be **structured** in sensible and comprehensible ways that simplify interaction and interpreting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful data structure in python (which exists in other programming languages) is the **array**, part of the `numpy` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays are **containers** of variables which can be accessed individually, looped over, or altered by functions. Examples of how to use them are shown below - experiment with different values and functionalities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 1: Simple array manipulations\n",
    "import numpy as np\n",
    "\n",
    "# Construct arrays of numbers\n",
    "array_1 = np.array([1,2,3,4])\n",
    "array_2 = np.array([5,6,7,8])\n",
    "\n",
    "print(\"Printing arrays\")\n",
    "print(array_1)\n",
    "print(array_2)\n",
    "\n",
    "# Operations for arrays\n",
    "print(\"\\nSimple array operations\")\n",
    "print(\"array_1 * array_2 = \", array_1 * array_2) # multiplies each element of array_1 by the corresponding element of array_2\n",
    "print(\"array_1 + array_2 = \", array_1 + array_2) # adds each element of array_1 to the corresponding element of array_2\n",
    "print(\"array_1 - array_2 = \", array_1 - array_2) # subtracts from each element of array_1 the corresponding element of array_2\n",
    "print(\"array_1 / array_2 = \", array_1 / array_2) # divides each element of array_1 by the corresponding element of array_2\n",
    "\n",
    "# Indexing for access to specific elements\n",
    "# The first element of the array is '0', the second is '1', ...\n",
    "print(\"\\nIndexing arrays\")\n",
    "print(\"array_1[0] = \", array_1[0])\n",
    "print(\"array_1[1] = \", array_1[1])\n",
    "print(\"array_1[2] = \", array_1[2])\n",
    "print(\"array_1[3] = \", array_1[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions can be used to modify components of arrays in many ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 2: The following function\n",
    "# transforms an array by multiplying\n",
    "# each element by its index\n",
    "import numpy as np\n",
    "\n",
    "def transform_array(array):\n",
    "    \n",
    "    # enumerate gives access to index\n",
    "    # and elements of a container\n",
    "    for idx, num in enumerate(array):\n",
    "        # inline multiplication multiplies\n",
    "        # the element by the index\n",
    "        array[idx] *= idx\n",
    "    \n",
    "    return array\n",
    "\n",
    "test_array = np.array([1,2,3,4])\n",
    "print(\"test array = \", test_array)\n",
    "print(\"transformed array = \", transform_array(test_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays can also be **multi-dimensional**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 2: Multi-dimensional arrays\n",
    "import numpy as np\n",
    "\n",
    "multi_array_1 = np.array([[1,2],[3,4]]) # This forms a 2x2 matrix!\n",
    "multi_array_2 = np.array([[5,6],[7,8]])\n",
    "\n",
    "# Multi-dimensional array operations:\n",
    "print(\"Multi-dimensional array operations\")\n",
    "print(\"multi_array_1 = \\n\", multi_array_1)\n",
    "print(\"\\nmulti_array_2 = \\n\", multi_array_2)\n",
    "print(\"\\nmulti_array_1 + multi_array_2 = \\n\", multi_array_1 + multi_array_2)\n",
    "\n",
    "# Indexing is more complicated:\n",
    "print(\"\\n---\\n\\nIndexing multi-dimensional arrays\")\n",
    "print(\"multi_array_1[0,0] = \\n\", multi_array_1[0,0])\n",
    "print(\"\\nmulti_array_1[0,1] = \\n\", multi_array_1[0,1])\n",
    "print(\"\\nmulti_array_1[1,0] = \\n\", multi_array_1[1,0])\n",
    "print(\"\\nmulti_array_1[1,1] = \\n\", multi_array_1[1,1])\n",
    "\n",
    "# The 'shape' function tells you the dimensions of the array\n",
    "print(\"\\n---\\n\\nShape of multi_array_1 = \", np.shape(multi_array_1)) # 2x2 matrix\n",
    "print(\"\\nShape of multi_array_2 = \", np.shape(multi_array_2)) # 2x2 matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Exercise 2a: Array transformations\n",
    "\n",
    "You are tasked to work with data to produce a histogram. For each bin the data is provided in the following format:\n",
    "\n",
    "$$[\\mathrm{ \\tt x\\_low \\quad x\\_high \\quad value \\quad error\\_minus \\quad error\\_plus}],$$\n",
    "\n",
    "where $\\mathrm{\\tt x\\_low, x\\_high}$ are the edges of the bins in $x$, $\\mathrm{\\tt value}$ is the value of the histogram at that point, and $\\mathrm{\\tt error\\_minus, error\\_plus}$ are the error bars in $y$ (i.e. the total uncertainty on $y=\\mathrm{\\tt value}$ is $\\mathrm{\\tt error\\_minus + error\\_plus}$).\n",
    "\n",
    "Produce a function that can add two histogram bins such that the **values** are added **linearly**, and the **errors** are combined **in quadrature** as discussed in the **introduction** notebook.\n",
    "\n",
    "**NOTE**: To combine two different contributions to the same bin, the $\\mathrm{\\tt x\\_low}$ and $\\mathrm{\\tt x\\_high}$ entries **MUST** match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to physically combine\n",
    "# two entries for the same histogram\n",
    "# bin in the stated format\n",
    "import numpy as np\n",
    "\n",
    "def combine_bin_contribs(bin_1, bin_2):\n",
    "    \n",
    "    # Hints:\n",
    "    # Consider how each component needs to\n",
    "    # transform when you add contributions\n",
    "    # to histogram bins.\n",
    "    \n",
    "    # should the [0] and [1] components\n",
    "    # change?\n",
    "    \n",
    "    # the values [2] should be combined\n",
    "    # by normal addition.\n",
    "    \n",
    "    # the statistical errors, [3] and [4],\n",
    "    # should each be combined in quadrature.\n",
    "    \n",
    "    return # Return value here\n",
    "\n",
    "contrib_1 = np.array([5, 10, 20, 3, 3])\n",
    "contrib_2 = np.array([5, 10, 10, 4, 4])\n",
    "print(combine_bin_contribs(contrib_1, contrib_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading external files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from experiments and from theoretical predictions can be imported in a variety of formats, often from external files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have included data in a format similar to the one from Exercise 2a, in the `.dat` files in the same folder as this notebook.\n",
    "\n",
    "We can read these in python in a number of different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 3: Reading files\n",
    "import numpy as np\n",
    "\n",
    "# Native python open() function\n",
    "# 'r' argument means 'read-only'\n",
    "f = open(\"lhc-data.dat\", \"r\")\n",
    "print(f.read())\n",
    "print(type(f.read()),\"\\n\\n\") # The native python 'open' function converts the file read to a string - not a useful format\n",
    "\n",
    "# Try with numpy - which has some smarter functions!\n",
    "example_histogram = np.genfromtxt(\"lhc-data.dat\", dtype=float)\n",
    "print(example_histogram)\n",
    "print(type(example_histogram)) # The method has converted the data to a numpy array!\n",
    "print(np.shape(example_histogram)) # The dimensions are right too - 20 bins, 5 columns for each bin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Exercise 2b: Complex array transformations\n",
    "\n",
    "Using the solution to Exercise 2a (i.e. not writing a new function from scratch), write a function that will add two histograms together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to physically combine\n",
    "# two histograms, using the function(s)\n",
    "# you have created earlier.\n",
    "import numpy as np\n",
    "\n",
    "def combine_histograms(hist_1, hist_2):\n",
    "    \n",
    "    # Hints:\n",
    "    # Can you use some of the tools\n",
    "    # we introduced in the previous\n",
    "    # notebook to enable the use of\n",
    "    # the function from Exercise 2a?\n",
    "    \n",
    "    return # Return combined histogram here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Combining theoretical predictions\n",
    "\n",
    "We have provided theoretical predictions for Higgs production in the base folder - including predictions from *High Energy Jets* ([HEJ](https://hej.hepforge.org/)) for the **strong-initiated process**, and predictions for *Vector Boson Fusion* or *VBF* (and related) which is the **weak-initiated** process.\n",
    "\n",
    "The composite prediction for Higgs production for this analysis will be the combination of both processes - you will need to **combine the histograms** for each component to produce a reasonable estimate of the Standard Model background.\n",
    "\n",
    "For each theoretical prediction we have provided a data file, each in the same format as the LHC experimental data you looked at above, called `hej-prediction.dat` and `vbf-prediction.dat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Exercise 2c: Reading files and combining predictions\n",
    "\n",
    "Using the solution to Exercise 2b, and the example for reading files, create a composite histogram by combining the HEJ and VBF predictions.\n",
    "This will give you the full Standard Model prediction and the total **theoretical** error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the two text files, and use your function from exercise 2b to combine them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You should have combined the theoretical predictions to produce a composite histogram (don't worry if not, the file `sm-prediction.dat`, is pre-made and available for you to use instead).\n",
    "\n",
    "Now you need to plot the results with the experimental data from `lhc-data.dat`.\n",
    "\n",
    "Remember the guidance from the introduction of what goes into a good plot!\n",
    "\n",
    "For the histogram, we require two lines:\n",
    "\n",
    "1. A scatter plot of the experimental data with **statistical uncertainty** plotted as **vertical error bars**.\n",
    "\n",
    "2. A composite Standard Model theoretical prediction with **theoretical uncertainties** (from $\\mathrm{\\tt error\\_minus, error\\_plus}$ ) plotted as **shaded bands**.\n",
    "\n",
    "Here is the example plot again for guidance:\n",
    "\n",
    "<img src=\"../introduction_notebook/example-distribution.png\" alt=\"Example distribution for Higgs production\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Exercise 2d: Producing the plot!\n",
    "\n",
    "Produce the required plot described above, pay attention to the special guidance from the introduction notebook.\n",
    "\n",
    "You can use the previous variables and functions you have created/used earlier.\n",
    "\n",
    "Some useful links for matplotlib:\n",
    "- [Error bars and scatters](https://matplotlib.org/stable/gallery/lines_bars_and_markers/errorbar_limits_simple.html#sphx-glr-gallery-lines-bars-and-markers-errorbar-limits-simple-py)\n",
    "- [Stair plots](https://matplotlib.org/stable/gallery/lines_bars_and_markers/stairs_demo.html)\n",
    "- [Shading regions between lines](https://matplotlib.org/stable/gallery/lines_bars_and_markers/fill_between_demo.html#sphx-glr-gallery-lines-bars-and-markers-fill-between-demo-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tell matplotlib to use LaTeX rendering, and a large font size\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif', size=18)\n",
    "\n",
    "# Load the data from the text files\n",
    "\n",
    "# Plot the experimental data with errors using the plt.errorbar function\n",
    "\n",
    "# Plot the central SM prediction using the plt.stairs function\n",
    "\n",
    "# Plot the SM uncertainties as shaded bands using the plt.fill_between function\n",
    "\n",
    "\n",
    "# You'll probably want to use log scaling on the y-axis (get rid of this line to see why!)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "# Add x and y axis labels\n",
    "\n",
    "# Now show the plot you have made\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data appears to show a **resonant feature** at $m_{jj}\\sim 850-900 \\,\\mathrm{GeV}$ !\n",
    "\n",
    "However, is this a statistically significant indication? The errors on the data are large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Exercise 2e: Statistical analysis\n",
    "\n",
    "Using the composite Standard Model prediction (with upper and lower bounds), determine whether the experimental data displays more than **five-sigma** tension with the theoretical SM background.\n",
    "\n",
    "This means using the loaded histogram data for each, and comparing the **values** in the 850-900 $\\mathrm{GeV}$ bin to see whether statistical fluctuation is exceedingly unlikely as an explanation of the bump in the data!\n",
    "\n",
    "Note that in our data we have **asymmetrical** errors, i.e. different positive and negative errors. There are various ways to deal with this kind of situation, but for simplicity here we can just average them to get a single experimental and theoretical error in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use everything you have worked with\n",
    "# above to perform the required statistical\n",
    "# analysis!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
